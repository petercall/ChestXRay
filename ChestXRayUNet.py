# -*- coding: utf-8 -*-
"""FinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c72E-ijPvBb27Fhd4u8V_oIu7J9jdoIM
"""

#Imports
import matplotlib.pyplot as plt                     #To display accuracy, loss, and chest x-ray images
import pandas as pd                                 #To load in the data from the JSON folder
import os                                           #To deal with file management
from google.colab import files                      #To load in files from my local machine
from PIL import Image                               #To convert the .png files into Tensors
import numpy as np                                  #Used in the validation function

from torchvision import transforms                  #To use transofrms.ToTensor()
from torch.utils.data import Dataset, DataLoader    #To define our custom data class
import torch                                        #To create tensors
import torch.nn as nn                               #To create a network
import torch.optim as optim                         #For the optimizer
from tqdm.auto import tqdm                          #To get a completion bar

#Set the device equal to cuda
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("cuda available")
else:
    device = torch.device("cpu")
    print("cpu available")

#Unzip the zipped image folders
# !unzip test.zip

#Define a dictionary of the different health defects in the x-rays (0 being no defect, i.e. a "clean" x-ray)
sym_dict = {"Clean" : 0,
            "Atelectasis" : 1,
            "Calcification" : 2,
            "Cardiomegaly" : 3,
            "Consolidation" : 4,
            "Diffuse Nodule" : 5,
            "Effusion" : 6,
            "Emphysema" : 7,
            "Fibrosis" : 8,
            "Fracture" : 9,
            "Mass" : 10,
            "Nodule" : 11,
            "Pleural Thickening" : 12,
            "Pneumothorax" : 13}

def encode_sym(sym_list):
    """
    This function encodes the possible symptoms according to the values in 'sym_dict'

    Inputs:
        sym_list - List : list of the things wrong with the x-ray as strings

    Returns:
        updated_sym_list - List : list of the things wrong with the x-ray as integers (0-13)
    """

    temp_sym_list = []

    if len(sym_list) == 0:
        temp_sym_list = [0]
    else:
        temp_sym_list = [sym_dict[sym] for sym in sym_list]
        temp_sym_list.sort()

    updated_sym_list = []

    for i in range(14):
        if i in temp_sym_list:
            updated_sym_list.append(1)
        else:
            updated_sym_list.append(0)


    return updated_sym_list

#Load in the data
df_test_labels = pd.read_json("ChestX_Det_test.json")

#Drop the boxes and polygons columns
df_test_labels = df_test_labels.drop(columns = ["boxes", "polygons"])

#Add a column of the symptoms as integers (0-13) rather than strings
df_test_labels["result"] = df_test_labels["syms"]

for i in range(df_test_labels.shape[0]):
    df_test_labels.at[i, "result"] = torch.tensor(encode_sym(list(set(df_test_labels.loc[i, "syms"]))), dtype = torch.long).to(device)

#Drop the column of the symptoms with the words
df_test_labels = df_test_labels.drop(columns = "syms").sort_values("file_name").reset_index(drop = True)

#Drop row 3 (so the row with index 2) from df_test_labels ("36266.png") because it has 4 channels for some weird reason
df_test_labels = df_test_labels.drop([2, 186, 221, 411, 447]).reset_index(drop = True)

#Create a new "image" column
df_test_labels["image"] = df_test_labels["result"]

transform = transforms.ToTensor()

path = "/content/test"
for i in range(df_test_labels.shape[0]):
    file = df_test_labels.iat[i, 0]
    file_path = os.path.join(path, file)
    img = Image.open(file_path)
    img_tens = transform(img).to(device)
    df_test_labels.at[i, "image"] = img_tens

#Define the dataset class
class images(Dataset):
    def __init__(self, df):
        super(images, self).__init__()
        self.df = df

    def __len__(self):
        return self.df.shape[0]

    def __getitem__(self, index):
        return self.df.at[index, "image"], self.df.at[index, "result"]

#Define your batch size
batch_size = 2

#Instantiate your training and test data
data = images(df_test_labels)
train_data, test_data = torch.utils.data.random_split(data, [450, 98])


# #Put your data into your data loader
train_loader =  DataLoader(train_data, batch_size = batch_size, shuffle = True)
test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = False)

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels, up = False):
        super(ConvBlock, self).__init__()
        self.up = up

        self.block = nn.Sequential(
            nn.Conv2d(in_channels, out_channels, stride = 1, padding = 1, kernel_size = 3),
            nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, stride = 1, padding = 1, kernel_size = 3),
            nn.ReLU()
        )

        self.up_samp = nn.ConvTranspose2d(out_channels, out_channels // 2, kernel_size = 2, stride = 2)


    def forward(self, input):
        x = self.block(input)

        if self.up:
            x = self.up_samp(x)

        return x


class Unet(nn.Module):
    def __init__(self):
        super(Unet, self).__init__()

        #Create your 4 down-sample blocks and define your max pool
        self.down1 = ConvBlock(1, 64)
        self.down2 = ConvBlock(64, 128)
        self.down3 = ConvBlock(128, 256)
        self.down4 = ConvBlock(256, 512)
        self.max = nn.MaxPool2d(kernel_size = 2, stride = 2)

        #Create your 4 up_sample blocks
        self.up1 = ConvBlock(512, 1024, True)
        self.up2 = ConvBlock(1024, 512, True)
        self.up3 = ConvBlock(512, 256, True)
        self.up4 = ConvBlock(256, 128, True)

        #Define your final block and final convolution, and final linear layer
        self.block_final = ConvBlock(128, 64)
        self.conv_final = nn.Conv2d(64, 14, kernel_size = 1, padding = 0, stride = 1)
        self.linear_final = nn.Linear(14*1024*1024, 2*14)

    def forward(self, input):
        b, c, h, w = input.size()

        #Put the input through the 4 down sample layers
        x1 = self.down1(input)
        x1max = self.max(x1)
        x2 = self.down2(x1max)
        x2max = self.max(x2)
        x3 = self.down3(x2max)
        x3max = self.max(x3)
        x4 = self.down4(x3max)
        x4max = self.max(x4)

        #put the input through the 4 up-sample layers
        x5 = self.up1(x4max)
        x6 = self.up2(torch.cat((x4, x5), dim = 1))
        x7 = self.up3(torch.cat((x3, x6), dim = 1))
        x8 = self.up4(torch.cat((x2, x7), dim = 1))

        #Do the final layers
        x9 = self.block_final(torch.cat((x1, x8), dim = 1))
        x10 = self.conv_final(x9)
        x11 = self.linear_final(x10.view(b, 14*1024*1024))

        return x11

#Define the custom loss function
objective1 = nn.CrossEntropyLoss()

class FunctionLoss(nn.Module):
    def __init__(self):
        super(FunctionLoss, self).__init__()

    def forward(self, y_hat, y):

        for i in range(14):
            y_i = y[:,i].view(-1)
            y_hat_i = y_hat[:,i:i+2]
            loss_i = objective1(y_hat_i, y_i)

            if i == 0:
                loss = loss_i
            else:
                loss += loss_i

        return loss

objective2 = FunctionLoss()

def validate():
    with torch.no_grad():

        validation_losses_epoch = []
        accuracy_epoch = []

        for x, y in test_loader:

            y_hat = model(x)
            loss = objective2(y_hat, y)

            validation_losses_epoch.append(loss.item())

            #Calculate the accuracy on the validation data
            for i in range(14):
                y_i = y[:,i].view(-1)
                y_hat_i = y_hat[:,i:i+2]

                model_labels = torch.argmax(y_hat_i, dim = 1)
                truth = [(guess == y_i[i]).item() for i, guess in enumerate(model_labels)]
                accuracy_epoch.append(np.mean(np.array(truth)))

    return np.mean(np.array(validation_losses_epoch)), np.mean(np.array(accuracy_epoch))

model = Unet().to(device)
optimizer = optim.Adam(model.parameters(), lr = .002)
objective3 = FunctionLoss()

#Create training and validation loss lists
training_losses = []
validation_losses = []
accuracy = []

num_epochs = 25
for epoch in tqdm(range(num_epochs)):

    training_losses_epoch = []

    for x, y in train_loader:

        optimizer.zero_grad()
        y_hat = model(x)
        loss = objective3(y_hat, y)

        #Append list to training epoch loss list
        training_losses_epoch.append(loss.item())

        loss.backward()
        optimizer.step()

    #Append mean training loss to training_losses
    training_losses.append(round(np.mean(np.array(training_losses_epoch)), 4))

    #Create the validation loop
    if epoch % 1 == 0:

        val_loss, acc = validate()
        validation_losses.append(round(val_loss, 4))
        accuracy.append(round(acc, 4))

        print(f"Training Loss: {training_losses[-1]} Validation Loss: {validation_losses[-1]} Accuracy: {accuracy[-1]}")

#Plot the Training and Validation Losses
plt.plot(training_losses, label = "Training Loss")
plt.plot(validation_losses, label = "Validation Loss", color = "orange")
plt.title("Training and Validation Losses")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

#Plot the Validation Accuracy
plt.plot(accuracy, label = "Validation Accuracy", color = "orange")
plt.title("Validation Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()